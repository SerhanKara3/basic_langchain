{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4238086",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    from langchain_core.runnables import RunnableLambda, RunnableWithMessageHistory\n",
    "    from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "    from langchain_core.prompt_values import ChatPromptValue\n",
    "\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    from langchain_groq import ChatGroq\n",
    "    from dotenv import load_dotenv\n",
    "    import requests\n",
    "    import os\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "except:\n",
    "    !pip install langchain langchain-google-genai langchain-groq\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    from langchain_core.runnables import RunnableLambda, RunnableWithMessageHistory\n",
    "    from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "    from langchain_core.prompt_values import ChatPromptValue\n",
    "\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    from langchain_groq import ChatGroq\n",
    "    from dotenv import load_dotenv\n",
    "    import requests\n",
    "    import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0e14a7",
   "metadata": {},
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbcc25b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "def call_apifreellm(prompt_value: ChatPromptValue) -> str:\n",
    "    message = prompt_value.to_string()\n",
    "\n",
    "    response = requests.post(\n",
    "        \"https://apifreellm.com/api/v1/chat\",\n",
    "        headers={\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {os.getenv('APIFREELLM_API_KEY')}\"\n",
    "        },\n",
    "        json={\"message\": message},\n",
    "        timeout=30\n",
    "    )\n",
    "\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    return data.get(\"response\") or data.get(\"message\")\n",
    "\n",
    "apifreellm = RunnableLambda(call_apifreellm)\n",
    "\n",
    "groq = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\"\n",
    ")\n",
    "\n",
    "gemma_3_1b = ChatGoogleGenerativeAI(\n",
    "    model=\"gemma-3-1b-it\",\n",
    "    temperature=0.7,\n",
    "    convert_system_message_to_human=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e44a4d4",
   "metadata": {},
   "source": [
    "### Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78ddc246",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You're an assistant who gives short and clear answers.\"),\n",
    "    (\"placeholder\", \"{history}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_chat_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "def build_memory_chain(llm):\n",
    "    base_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    return RunnableWithMessageHistory(\n",
    "        base_chain,\n",
    "        get_chat_history,\n",
    "        input_messages_key=\"question\",\n",
    "        history_messages_key=\"history\",\n",
    "    )\n",
    "\n",
    "afl_chain = build_memory_chain(apifreellm)\n",
    "groq_chain = build_memory_chain(groq)\n",
    "gemma_chain = build_memory_chain(gemma_3_1b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e097afb",
   "metadata": {},
   "source": [
    "### Short Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dabd5d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a tool that makes it easier to build applications using large language models. \n",
      "\n",
      "Think of it as a kit for creating intelligent applications! \n",
      "LangChain was created by Beijing Academy of Artificial Intelligence (BAAI).\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"user_1\"}}\n",
    "\n",
    "response = gemma_chain.invoke({\"question\": \"What is LangChain?\"}, config=config)\n",
    "print(response)\n",
    "\n",
    "response = gemma_chain.invoke({\"question\": \"Who created it?\"}, config=config)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ad98d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a tool that helps build apps using AI. It was made by a group called BAAI.\n"
     ]
    }
   ],
   "source": [
    "response = groq_chain.invoke({\"question\": \"Explain it more simply.\"}, config=config)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f61183a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: You can use LangChain to build a chatbot that answers customer questions, using a large language model to understand and respond to user queries.\n"
     ]
    }
   ],
   "source": [
    "response = groq_chain.invoke({\"question\": \"Give an example use case.\"}, config=config)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff8de9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: A company uses LangChain to build an AI-powered research assistant that can summarize complex documents, answer technical questions, and even generate code snippets, all using a large language model integrated with their existing knowledge base and workflow tools.\n"
     ]
    }
   ],
   "source": [
    "response = groq_chain.invoke({\"question\": \"Make it more challenging\"}, config=config)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a029e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
