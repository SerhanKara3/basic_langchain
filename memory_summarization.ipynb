{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c11975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import os\n",
    "    from typing import Dict\n",
    "    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "    from langchain_core.runnables import RunnableWithMessageHistory\n",
    "    from langchain_core.chat_history import BaseChatMessageHistory\n",
    "    from langchain_core.messages import HumanMessage, AIMessage, get_buffer_string\n",
    "    from langchain_groq import ChatGroq\n",
    "except:\n",
    "    !pip install langchain langchain-groq\n",
    "    import os\n",
    "    from typing import Dict\n",
    "    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "    from langchain_core.runnables import RunnableWithMessageHistory\n",
    "    from langchain_core.chat_history import BaseChatMessageHistory\n",
    "    from langchain_core.messages import HumanMessage, AIMessage, get_buffer_string\n",
    "    from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f99466",
   "metadata": {},
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acb6bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "chat_llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368fb6fc",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8679bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a precise assistant. \n",
    "    IMPORTANT: You will be provided with a 'Conversation summary' in the chat history. \n",
    "    This summary contains the DEFINITIVE facts about the user (Name, Preferences, etc.).\n",
    "    Prioritize the information in the summary over previous individual messages.\n",
    "    If the summary says the user's name is x, then their name is x, even if the recent messages are brief.\n",
    "    In this case, rely on the summary information and avoid referring the user to the summary or recent conversations.\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Analyze the following conversation history and extract critical user information to create a technical memory log.\n",
    "    RULE 1: Persistent data like Name, Profession, and Preferences must NEVER be deleted or marked as 'unknown'.\n",
    "    RULE 2: Keep the information in a list format (Entity: Detail).\n",
    "    RULE 3: If a previous summary exists, merge old information with new data; do not lose previous context.\"\"\"),\n",
    "    (\"human\", \"Current Memory and New Messages:\\n\\n{history_text}\\n\\nPlease generate an updated and complete summary.\")\n",
    "])\n",
    "\n",
    "summary_chain = summary_prompt | summary_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bfd54b",
   "metadata": {},
   "source": [
    "### Evaluative chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbc8ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluativeSummarizingHistory(BaseChatMessageHistory):\n",
    "    def __init__(self, max_messages: int = 5, keep_last: int = 2):\n",
    "        self.messages = []\n",
    "        self.max_messages = max_messages\n",
    "        self.keep_last = keep_last\n",
    "\n",
    "    def add_message(self, message):\n",
    "        self.messages.append(message)\n",
    "        interaction_count = len([\n",
    "            m for m in self.messages \n",
    "            if not (isinstance(m, AIMessage) and \"Conversation summary:\" in m.content)\n",
    "        ])\n",
    "        \n",
    "        if interaction_count > self.max_messages:\n",
    "            self._summarize()\n",
    "\n",
    "    def _summarize(self):\n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(\"--- [CRITICAL MEMORY UPDATE TRIGGERED] ---\")\n",
    "        \n",
    "        history_text = get_buffer_string(self.messages)\n",
    "        summary_output = summary_chain.invoke({\"history_text\": history_text})\n",
    "        new_summary = summary_output.content.strip()\n",
    "        \n",
    "        print(f\"UPDATED MEMORY LOG:\\n{new_summary}\")\n",
    "        \n",
    "        self.messages = [\n",
    "            AIMessage(content=f\"IMPORTANT CONTEXT - Conversation summary: {new_summary}\")\n",
    "        ] + self.messages[-self.keep_last:]\n",
    "        \n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    def clear(self):\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48282ca8",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3cea07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STARTING MEMORY QUALITY EVALUATION ===\n",
      "\n",
      "User: My name is Serhan, I am a Python developer.\n",
      "AI: Hello Serhan, it's nice to meet you. As a Python developer, you must be familiar with a wide range of libraries and frameworks. What specific areas of Python development are you interested in or currently working on?\n",
      "\n",
      "User: I am currently working on memory management in LangChain using Llama and Gemma.\n",
      "AI: That sounds like a fascinating project, Serhan. LangChain is an exciting framework for building AI applications, and Llama and Gemma are powerful tools for large language models. Memory management can be a challenging aspect of working with these models, given their complexity and computational requirements.\n",
      "\n",
      "What specific challenges are you facing with memory management in your project, and how are you approaching optimization and efficiency? Are you exploring techniques like caching, pruning, or distributed computing to mitigate memory constraints?\n",
      "\n",
      "User: Which language am I an expert in?\n",
      "\n",
      "============================================================\n",
      "--- [CRITICAL MEMORY UPDATE TRIGGERED] ---\n",
      "UPDATED MEMORY LOG:\n",
      "Based on the conversation history, the updated technical memory log is:\n",
      "\n",
      "* Name: Serhan\n",
      "* Profession: Python developer\n",
      "* Areas of Interest: Memory management in LangChain using Llama and Gemma\n",
      "* Current Project: Working on memory management in LangChain using Llama and Gemma\n",
      "* Expertise: Python \n",
      "* Previous Conversations: Discussed LangChain, Llama, Gemma, and memory management challenges, and was asked about optimization techniques such as caching, pruning, or distributed computing.\n",
      "============================================================\n",
      "\n",
      "AI: You are an expert in Python, Serhan.\n",
      "\n",
      "User: My favorite food is Iskender, please don't forget that.\n",
      "\n",
      "============================================================\n",
      "--- [CRITICAL MEMORY UPDATE TRIGGERED] ---\n",
      "UPDATED MEMORY LOG:\n",
      "Based on the conversation history, the updated technical memory log is:\n",
      "\n",
      "* Name: Serhan\n",
      "* Profession: Python developer\n",
      "* Areas of Interest: Memory management in LangChain using Llama and Gemma\n",
      "* Current Project: Working on memory management in LangChain using Llama and Gemma\n",
      "* Expertise: Python \n",
      "* Previous Conversations: Discussed LangChain, Llama, Gemma, and memory management challenges, and was asked about optimization techniques such as caching, pruning, or distributed computing.\n",
      "* Favorite Food: Iskender\n",
      "\n",
      "The new information about Serhan's favorite food has been added to the technical memory log, while maintaining the existing information about his name, profession, areas of interest, current project, expertise, and previous conversations.\n",
      "============================================================\n",
      "\n",
      "AI: I've taken note of that, Serhan. Your favorite food is Iskender. I'll keep that in mind for any future conversations.\n",
      "\n",
      "User: I have a bachelor's degree in mathematics and am pursuing a master's degree in computer science.\n",
      "AI: I've updated my information about you, Serhan. You have a bachelor's degree in mathematics and are currently pursuing a master's degree in computer science. This is in addition to your profession as a Python developer, and your interests in memory management in LangChain using Llama and Gemma. And, of course, your favorite food is still Iskender.\n",
      "\n",
      "User: What do you know about me so far? What is my name, what do I do, and what do I like?\n",
      "\n",
      "============================================================\n",
      "--- [CRITICAL MEMORY UPDATE TRIGGERED] ---\n",
      "UPDATED MEMORY LOG:\n",
      "Based on the conversation history, the updated technical memory log is:\n",
      "\n",
      "* Name: Serhan\n",
      "* Profession: Python developer\n",
      "* Education: \n",
      "  * Bachelor's degree: Mathematics\n",
      "  * Master's degree: Computer Science (pursuing)\n",
      "* Areas of Interest: Memory management in LangChain using Llama and Gemma\n",
      "* Current Project: Working on memory management in LangChain using Llama and Gemma\n",
      "* Expertise: Python \n",
      "* Previous Conversations: Discussed LangChain, Llama, Gemma, and memory management challenges, and was asked about optimization techniques such as caching, pruning, or distributed computing.\n",
      "* Favorite Food: Iskender\n",
      "\n",
      "This updated technical memory log includes the new information about Serhan's educational background while maintaining the existing information about his name, profession, areas of interest, current project, expertise, previous conversations, and favorite food.\n",
      "============================================================\n",
      "\n",
      "AI: Serhan, I know that your name is indeed Serhan. You are a Python developer by profession. You are currently pursuing a master's degree in computer science, having already completed a bachelor's degree in mathematics. Your areas of interest include memory management in LangChain using Llama and Gemma. As for your personal preferences, I know that your favorite food is Iskender.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "store: Dict[str, BaseChatMessageHistory] = {}\n",
    "\n",
    "def get_chat_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = EvaluativeSummarizingHistory(max_messages=4, keep_last=2)\n",
    "    return store[session_id]\n",
    "\n",
    "full_chain = RunnableWithMessageHistory(\n",
    "    chat_prompt | chat_llm,\n",
    "    get_chat_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"github_demo_session\"}}\n",
    "\n",
    "test_questions = [\n",
    "    \"My name is Serhan, I am a Python developer.\",\n",
    "    \"I am currently working on memory management in LangChain using Llama and Gemma.\",\n",
    "    \"Which language am I an expert in?\",\n",
    "    \"My favorite food is Iskender, please don't forget that.\",\n",
    "    \"I have a bachelor's degree in mathematics and am pursuing a master's degree in computer science.\",\n",
    "    \"What do you know about me so far? What is my name, what do I do, and what do I like?\"\n",
    "]\n",
    "\n",
    "print(\"=== STARTING MEMORY QUALITY EVALUATION ===\\n\")\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"User: {q}\")\n",
    "    res = full_chain.invoke({\"question\": q}, config=config)\n",
    "    print(f\"AI: {res.content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa30ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
